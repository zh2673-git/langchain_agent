"""
Chainæ–¹å¼çš„Agentå®ç°
ä½¿ç”¨LangChainçš„Chainç»„åˆæ–¹å¼å®ç°å¤šè½®å¯¹è¯ã€å·¥å…·è°ƒç”¨ã€è®°å¿†ç®¡ç†

æ ¸å¿ƒç‰¹ç‚¹ï¼š
1. ä½¿ç”¨Runnableæ¥å£ç»„åˆå„ç§ç»„ä»¶
2. æ”¯æŒæµå¼è¾“å‡º
3. çµæ´»çš„é“¾å¼ç»„åˆ
4. å®Œå…¨åŸºäºLangChainæ ‡å‡†å®ç°
"""

import asyncio
from typing import Dict, Any, List, Optional, AsyncIterator
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel
try:
    from langchain_ollama import ChatOllama
except ImportError:
    from langchain_community.chat_models import ChatOllama

from ..utils.logger import get_logger
from ..tools.tool_service import get_tool_service, initialize_tool_service
from ..memory.memory_service import get_memory_service, initialize_memory_service
from ..config import config

logger = get_logger(__name__)


class ChainAgent:
    """
    åŸºäºChainçš„Agentå®ç°
    
    ä½¿ç”¨LangChainçš„Runnableæ¥å£ç»„åˆå®ç°ï¼š
    1. å¤šè½®å¯¹è¯ç®¡ç†
    2. å·¥å…·è°ƒç”¨ï¼ˆé€šè¿‡æç¤ºè¯æ–¹å¼ï¼‰
    3. è®°å¿†ç®¡ç†
    4. æµå¼è¾“å‡ºæ”¯æŒ
    """
    
    def __init__(self, provider: str = "ollama", model: str = "qwen2.5:7b"):
        self.provider = provider
        self.model = model
        
        # æ ¸å¿ƒç»„ä»¶
        self.llm = None
        self.tool_service = get_tool_service()
        self.memory_service = get_memory_service()
        
        # Chainç»„ä»¶
        self.conversation_chain = None
        self.tool_chain = None
        self.main_chain = None
        
        # çŠ¶æ€
        self.initialized = False
        
        logger.info(f"ChainAgent created for {provider}:{model}")

    async def _initialize_llm(self) -> bool:
        """åˆå§‹åŒ–LLM"""
        try:
            if self.provider == "ollama":
                self.llm = ChatOllama(
                    model=self.model,
                    temperature=0.7,
                    streaming=True,
                    base_url=config.OLLAMA_BASE_URL
                )
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")

            logger.info(f"LLM initialized: {self.provider}:{self.model}")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            return False

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–Agent"""
        try:
            # 1. åˆå§‹åŒ–LLM
            success = await self._initialize_llm()
            if not success:
                return False

            # 2. åˆå§‹åŒ–æœåŠ¡
            await initialize_memory_service()
            await initialize_tool_service(self.llm, self.provider, self.model)
            
            # 3. æ„å»ºChain
            await self._build_chains()

            self.initialized = True
            logger.info("ChainAgent initialized successfully")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize ChainAgent: {e}")
            return False
    
    async def _build_chains(self):
        """æ„å»ºChainç»„åˆ"""
        
        # 1. å¯¹è¯Chain - å¤„ç†æ™®é€šå¯¹è¯
        conversation_prompt = ChatPromptTemplate.from_messages([
            ("system", self._get_system_prompt()),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}")
        ])
        
        self.conversation_chain = (
            RunnablePassthrough.assign(
                chat_history=RunnableLambda(self._get_chat_history),
                tools_description=RunnableLambda(lambda x: self.tool_service.get_tools_description())
            )
            | conversation_prompt
            | self.llm
            | StrOutputParser()
        )
        
        # 2. å·¥å…·Chain - å¤„ç†å·¥å…·è°ƒç”¨
        self.tool_chain = RunnableLambda(self._handle_tool_calls)
        
        # 2. å·¥å…·Chain - å¤„ç†å·¥å…·è°ƒç”¨
        self.tool_chain = RunnableLambda(self._handle_tool_calls)

        # 3. ä¸»Chain - ç»„åˆå¯¹è¯å’Œå·¥å…·è°ƒç”¨
        async def process_main_chain(inputs):
            # å…ˆæ‰§è¡Œå¯¹è¯
            response = await self.conversation_chain.ainvoke(inputs)
            # ç„¶åå¤„ç†å·¥å…·è°ƒç”¨
            return await self._process_response({"response": response})

        self.main_chain = RunnableLambda(process_main_chain)
        
        logger.info("Chains built successfully")
    
    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥è¿›è¡Œå¯¹è¯å¹¶ä½¿ç”¨å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ã€‚

å½“ä½ éœ€è¦ä½¿ç”¨å·¥å…·æ—¶ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ï¼š
TOOL_CALL: å·¥å…·åç§°
å‚æ•°: {{"å‚æ•°å": "å‚æ•°å€¼"}}

å¯ç”¨å·¥å…·ï¼š
{tools_description}

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜é€‰æ‹©åˆé€‚çš„å·¥å…·ï¼Œæˆ–ç›´æ¥å›ç­”é—®é¢˜ã€‚"""
    
    def _get_chat_history(self, inputs: Dict[str, Any]) -> List[BaseMessage]:
        """è·å–èŠå¤©å†å²"""
        session_id = inputs.get("session_id")
        if not session_id:
            return []
        
        try:
            # é€šè¿‡è®°å¿†æœåŠ¡è·å–èŠå¤©å†å²
            return self.memory_service.get_chat_history_sync(session_id, limit=10)
        except Exception as e:
            logger.error(f"Failed to get chat history: {e}")
            return []
    
    async def _process_response(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å“åº”ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨"""
        response = inputs.get("response", "")

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
        if "TOOL_CALL:" in response:
            return await self._handle_tool_calls(inputs)
        else:
            return {
                "content": response,
                "tool_calls": [],
                "success": True
            }
    
    async def _handle_tool_calls(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å·¥å…·è°ƒç”¨"""
        response = inputs.get("response", "")
        tool_calls = []
        
        try:
            # è§£æå·¥å…·è°ƒç”¨
            lines = response.split('\n')
            i = 0
            while i < len(lines):
                line = lines[i].strip()
                if line.startswith("TOOL_CALL:"):
                    tool_name = line.replace("TOOL_CALL:", "").strip()
                    
                    # æŸ¥æ‰¾å‚æ•°
                    params = {}
                    if i + 1 < len(lines) and lines[i + 1].strip().startswith("å‚æ•°:"):
                        try:
                            import json
                            params_str = lines[i + 1].replace("å‚æ•°:", "").strip()
                            params = json.loads(params_str)
                        except:
                            params = {}
                    
                    # æ‰§è¡Œå·¥å…·
                    result = await self.tool_service.execute_tool(tool_name, **params)
                    tool_calls.append({
                        "tool": tool_name,
                        "input": params,
                        "result": result.get("result", ""),
                        "success": result.get("success", False)
                    })
                
                i += 1
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œç”ŸæˆåŒ…å«å·¥å…·ç»“æœçš„å“åº”
            if tool_calls:
                tool_results = []
                for call in tool_calls:
                    tool_results.append(f"å·¥å…· {call['tool']} æ‰§è¡Œç»“æœï¼š{call['result']}")
                
                final_response = response + "\n\n" + "\n".join(tool_results)
            else:
                final_response = response
            
            return {
                "content": final_response,
                "tool_calls": tool_calls,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"Tool call handling failed: {e}")
            return {
                "content": response,
                "tool_calls": [],
                "success": True,
                "error": str(e)
            }
    
    async def chat(self, message: str, session_id: str = None, **kwargs) -> Dict[str, Any]:
        """èŠå¤©æ–¹æ³•"""
        try:
            if not self.initialized:
                return {"success": False, "error": "Agent not initialized"}
            
            # å‡†å¤‡è¾“å…¥
            input_data = {
                "input": message,
                "session_id": session_id
            }
            
            # æ‰§è¡Œä¸»Chain
            result = await self.main_chain.ainvoke(input_data)
            
            # ä¿å­˜åˆ°å†…å­˜
            if session_id:
                await self.memory_service.add_message(
                    session_id, HumanMessage(content=message)
                )
                await self.memory_service.add_message(
                    session_id, AIMessage(content=result["content"])
                )
            
            return {
                "success": True,
                "content": result["content"],
                "tool_calls": result.get("tool_calls", []),
                "model_info": {
                    "provider": self.provider,
                    "model": self.model,
                    "type": "chain"
                }
            }
            
        except Exception as e:
            logger.error(f"Chat failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "content": f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}"
            }
    
    async def chat_stream(self, message: str, session_id: str = None, **kwargs) -> AsyncIterator[Dict[str, Any]]:
        """æµå¼èŠå¤©"""
        try:
            if not self.initialized:
                yield {"success": False, "error": "Agent not initialized", "done": True}
                return
            
            input_data = {
                "input": message,
                "session_id": session_id
            }
            
            # æµå¼æ‰§è¡Œ
            full_response = ""
            async for chunk in self.conversation_chain.astream(input_data):
                if chunk:
                    full_response += chunk
                    yield {
                        "success": True,
                        "content": chunk,
                        "done": False
                    }
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            if "TOOL_CALL:" in full_response:
                tool_result = await self._handle_tool_calls({"response": full_response})
                if tool_result.get("tool_calls"):
                    for call in tool_result["tool_calls"]:
                        yield {
                            "success": True,
                            "content": f"\nğŸ”§ æ‰§è¡Œå·¥å…·: {call['tool']} -> {call['result']}",
                            "done": False,
                            "tool_call": call
                        }
            
            yield {"success": True, "content": "", "done": True}
            
            # ä¿å­˜åˆ°å†…å­˜
            if session_id:
                await self.memory_service.add_message(
                    session_id, HumanMessage(content=message)
                )
                await self.memory_service.add_message(
                    session_id, AIMessage(content=full_response)
                )
            
        except Exception as e:
            logger.error(f"Stream chat failed: {e}")
            yield {
                "success": False,
                "error": str(e),
                "content": f"æµå¼å¤„ç†å‡ºé”™: {str(e)}",
                "done": True
            }
    
    async def get_info(self) -> Dict[str, Any]:
        """è·å–Agentä¿¡æ¯"""
        return {
            "type": "chain",
            "provider": self.provider,
            "model": self.model,
            "initialized": self.initialized,
            "supports_tools": True,
            "tools_count": len(self.tool_service.get_tools()),
            "memory_enabled": True
        }
    
    async def clear_memory(self, session_id: str) -> bool:
        """æ¸…é™¤å†…å­˜"""
        return await self.memory_service.clear_session(session_id)

    async def switch_model(self, new_model: str) -> bool:
        """åˆ‡æ¢åº•å±‚æ¨¡å‹"""
        try:
            old_model = self.model
            self.model = new_model

            # é‡æ–°åˆå§‹åŒ–LLM
            success = await self._initialize_llm()
            if success:
                # é‡æ–°æ„å»ºchains
                await self._build_chains()
                logger.info(f"Successfully switched from {old_model} to {new_model}")
                return True
            else:
                # æ¢å¤åŸæ¨¡å‹
                self.model = old_model
                logger.error(f"Failed to switch to {new_model}, reverted to {old_model}")
                return False

        except Exception as e:
            logger.error(f"Error switching model: {e}")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "current_model": self.model,
            "provider": self.provider,
            "agent_type": "chain",
            "initialized": self.initialized
        }
