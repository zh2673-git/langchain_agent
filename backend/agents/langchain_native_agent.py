"""
åŸºäºŽLangChainæºç çš„åŽŸç”ŸAgentå®žçŽ°
å®Œå…¨æŒ‰ç…§LangChainå®˜æ–¹æºç å’Œæœ€ä½³å®žè·µå®žçŽ°
"""

import asyncio
from typing import Dict, Any, List, Optional, AsyncIterator, Union
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.tools import BaseTool
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.agents.format_scratchpad import format_to_tool_messages
from langchain.agents.output_parsers import ToolsAgentOutputParser
from langchain_community.chat_models import ChatOllama

from ..utils.logger import get_logger
from ..memory.memory_manager import MemoryManager
from ..tools.tool_manager import ToolManager
from ..config import config

logger = get_logger(__name__)


class LangChainNativeAgent:
    """
    åŸºäºŽLangChainæºç çš„åŽŸç”ŸAgentå®žçŽ°
    
    å®Œå…¨æŒ‰ç…§LangChainå®˜æ–¹æ–‡æ¡£å’Œæºç å®žçŽ°ï¼š
    1. ä½¿ç”¨LangChainçš„æ ‡å‡†RunnableæŽ¥å£
    2. æ”¯æŒåŽŸç”Ÿå·¥å…·è°ƒç”¨ï¼ˆå¦‚æžœæ¨¡åž‹æ”¯æŒï¼‰
    3. æ”¯æŒæµå¼è¾“å‡º
    4. å®Œå…¨å…¼å®¹LangChainç”Ÿæ€ç³»ç»Ÿ
    """
    
    def __init__(self, provider: str = "ollama", model: str = "qwen2.5:7b"):
        self.provider = provider
        self.model = model
        self.model_name = model
        
        # LangChainç»„ä»¶
        self.llm = None
        self.memory_manager = None
        self.tool_manager = None
        
        # Agentç»„ä»¶
        self.agent = None
        self.agent_executor = None
        self.chain = None
        
        # çŠ¶æ€
        self.initialized = False
        
        logger.info(f"LangChainNativeAgent created for {provider}:{model}")
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–Agent - å®Œå…¨æŒ‰ç…§LangChainæœ€ä½³å®žè·µ"""
        try:
            # 1. åˆå§‹åŒ–LLM - ä½¿ç”¨LangChainæ ‡å‡†æ–¹å¼
            if self.provider == "ollama":
                self.llm = ChatOllama(
                    model=self.model,
                    temperature=0.7,
                    streaming=True  # å¯ç”¨æµå¼è¾“å‡º
                )
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
            
            # 2. åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨
            self.memory_manager = MemoryManager()
            await self.memory_manager.initialize()
            
            # 3. åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨
            self.tool_manager = ToolManager(self.llm, self.provider, self.model)
            await self._load_tools()
            
            # 4. æž„å»ºAgent - ä½¿ç”¨LangChainæ ‡å‡†æ–¹å¼
            await self._build_agent()
            
            self.initialized = True
            logger.info("LangChainNativeAgent initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize LangChainNativeAgent: {e}")
            return False
    
    async def _load_tools(self):
        """åŠ è½½å·¥å…· - ä½¿ç”¨LangChainæ ‡å‡†å·¥å…·æŽ¥å£"""
        from ..tools.tool_loader import ToolLoader

        tool_loader = ToolLoader()
        tools = await tool_loader.load_all_tools()
        for tool in tools:
            self.tool_manager.add_tool(tool)

        logger.info(f"Loaded {len(tools)} tools")
    
    async def _build_agent(self):
        """æž„å»ºAgent - å®Œå…¨æŒ‰ç…§LangChainæºç å®žçŽ°"""
        
        # æ£€æŸ¥æ¨¡åž‹æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
        supports_tools = config.model_supports_tools(self.provider, self.model)
        
        if supports_tools and self.tool_manager.get_langchain_tools():
            # ä½¿ç”¨åŽŸç”Ÿå·¥å…·è°ƒç”¨Agent
            await self._build_tool_calling_agent()
        else:
            # ä½¿ç”¨åŸºç¡€å¯¹è¯é“¾
            await self._build_conversation_chain()
    
    async def _build_tool_calling_agent(self):
        """æž„å»ºå·¥å…·è°ƒç”¨Agent - æŒ‰ç…§LangChainå®˜æ–¹å®žçŽ°"""
        
        # èŽ·å–LangChainå·¥å…·
        tools = self.tool_manager.get_langchain_tools()
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿ - æŒ‰ç…§LangChainæ ‡å‡†
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant. Use tools when appropriate."),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        # ç»‘å®šå·¥å…·åˆ°LLM - LangChainæ ‡å‡†æ–¹å¼
        llm_with_tools = self.llm.bind_tools(tools)
        
        # åˆ›å»ºAgent - ä½¿ç”¨LangChainçš„create_tool_calling_agent
        self.agent = create_tool_calling_agent(llm_with_tools, tools, prompt)
        
        # åˆ›å»ºAgentExecutor - LangChainæ ‡å‡†æ‰§è¡Œå™¨
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=tools,
            verbose=True,
            return_intermediate_steps=True,
            handle_parsing_errors=True
        )
        
        logger.info("Tool calling agent built successfully")
    
    async def _build_conversation_chain(self):
        """æž„å»ºå¯¹è¯é“¾ - æŒ‰ç…§LangChain RunnableæŽ¥å£"""
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant."),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}")
        ])
        
        # æž„å»ºé“¾ - ä½¿ç”¨LangChainçš„RunnableæŽ¥å£
        self.chain = (
            RunnablePassthrough.assign(
                chat_history=lambda x: self._get_chat_history(x.get("session_id"))
            )
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        logger.info("Conversation chain built successfully")
    
    def _get_chat_history(self, session_id: str) -> List[BaseMessage]:
        """èŽ·å–èŠå¤©åŽ†å² - è¿”å›žLangChainæ ‡å‡†Messageå¯¹è±¡ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        if not session_id:
            return []

        try:
            # ä½¿ç”¨ç®€åŒ–çš„æœ¬åœ°åŽ†å²è®°å½•ï¼Œé¿å…asyncé—®é¢˜
            if hasattr(self, '_local_history'):
                history = self._local_history.get(session_id, [])
            else:
                self._local_history = {}
                history = []

            return history[-10:]  # æœ€è¿‘10æ¡æ¶ˆæ¯
        except Exception as e:
            logger.error(f"Failed to get chat history: {e}")
            return []
    
    async def chat(self, message: str, session_id: str = None, **kwargs) -> Dict[str, Any]:
        """èŠå¤©æ–¹æ³• - å®Œå…¨æŒ‰ç…§LangChainæ ‡å‡†å®žçŽ°"""
        try:
            if not self.initialized:
                return {"success": False, "error": "Agent not initialized"}
            
            # å‡†å¤‡è¾“å…¥ - LangChainæ ‡å‡†æ ¼å¼
            input_data = {
                "input": message,
                "session_id": session_id
            }
            
            # æ‰§è¡Œ - ä½¿ç”¨LangChainæ ‡å‡†æ–¹å¼
            if self.agent_executor:
                # å·¥å…·è°ƒç”¨æ¨¡å¼
                result = await self.agent_executor.ainvoke(input_data)
                response_content = result["output"]
                intermediate_steps = result.get("intermediate_steps", [])
                
                # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
                tool_calls = []
                for step in intermediate_steps:
                    if len(step) >= 2:
                        action, observation = step[0], step[1]
                        tool_calls.append({
                            "tool": action.tool,
                            "input": action.tool_input,
                            "result": str(observation),
                            "success": True
                        })
                
            else:
                # å¯¹è¯æ¨¡å¼
                result = await self.chain.ainvoke(input_data)
                response_content = result
                tool_calls = []
            
            # ä¿å­˜åˆ°å†…å­˜ - ä½¿ç”¨LangChainæ ‡å‡†Messageå¯¹è±¡
            if session_id:
                await self.memory_manager.add_message(
                    session_id, HumanMessage(content=message)
                )
                await self.memory_manager.add_message(
                    session_id, AIMessage(content=response_content)
                )
            
            return {
                "success": True,
                "content": response_content,
                "tool_calls": tool_calls,
                "thinking_process": "",  # å¯ä»¥ä»Žä¸­é—´æ­¥éª¤æå–
                "model_info": {
                    "provider": self.provider,
                    "model": self.model,
                    "supports_tools": bool(self.agent_executor)
                }
            }
            
        except Exception as e:
            logger.error(f"Chat failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "content": f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}"
            }
    
    async def chat_stream(self, message: str, session_id: str = None, **kwargs) -> AsyncIterator[Dict[str, Any]]:
        """æµå¼èŠå¤© - å®Œå…¨æŒ‰ç…§LangChainæµå¼æŽ¥å£å®žçŽ°"""
        try:
            if not self.initialized:
                yield {"success": False, "error": "Agent not initialized", "done": True}
                return
            
            # å‡†å¤‡è¾“å…¥
            input_data = {
                "input": message,
                "session_id": session_id
            }
            
            # æµå¼æ‰§è¡Œ - ä½¿ç”¨LangChainçš„astreamæ–¹æ³•
            if self.agent_executor:
                # Agentæµå¼æ‰§è¡Œ
                async for chunk in self.agent_executor.astream(input_data):
                    if "output" in chunk:
                        yield {
                            "success": True,
                            "content": chunk["output"],
                            "done": False
                        }
                    elif "intermediate_step" in chunk:
                        # å·¥å…·è°ƒç”¨æ­¥éª¤
                        step = chunk["intermediate_step"]
                        yield {
                            "success": True,
                            "content": f"ðŸ”§ è°ƒç”¨å·¥å…·: {step[0].tool}",
                            "done": False,
                            "tool_call": {
                                "tool": step[0].tool,
                                "input": step[0].tool_input
                            }
                        }
                
                yield {"success": True, "content": "", "done": True}
                
            else:
                # é“¾å¼æµå¼æ‰§è¡Œ
                full_response = ""
                async for chunk in self.chain.astream(input_data):
                    if chunk:
                        full_response += chunk
                        yield {
                            "success": True,
                            "content": chunk,
                            "done": False
                        }
                
                yield {"success": True, "content": "", "done": True}
                
                # ä¿å­˜åˆ°å†…å­˜
                if session_id:
                    await self.memory_manager.add_message(
                        session_id, HumanMessage(content=message)
                    )
                    await self.memory_manager.add_message(
                        session_id, AIMessage(content=full_response)
                    )
            
        except Exception as e:
            logger.error(f"Stream chat failed: {e}")
            yield {
                "success": False,
                "error": str(e),
                "content": f"æµå¼å¤„ç†å‡ºé”™: {str(e)}",
                "done": True
            }
    
    async def get_info(self) -> Dict[str, Any]:
        """èŽ·å–Agentä¿¡æ¯"""
        return {
            "type": "langchain_native",
            "provider": self.provider,
            "model": self.model,
            "initialized": self.initialized,
            "supports_tools": bool(self.agent_executor),
            "tools_count": len(self.tool_manager.list_tools()) if self.tool_manager else 0,
            "memory_enabled": bool(self.memory_manager)
        }
    
    async def clear_memory(self, session_id: str) -> bool:
        """æ¸…é™¤å†…å­˜"""
        if self.memory_manager:
            return await self.memory_manager.clear_session(session_id)
        return False
