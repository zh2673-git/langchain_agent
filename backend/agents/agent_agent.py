"""
åŸºäºLangChainåŸç”ŸAgentå®ç°
ä½¿ç”¨create_tool_calling_agentå’ŒAgentExecutorå®ç°æ ‡å‡†çš„LangChain Agent

æ ¸å¿ƒç‰¹ç‚¹ï¼š
1. ä½¿ç”¨LangChainæ ‡å‡†çš„create_tool_calling_agent
2. æ”¯æŒåŸç”Ÿå·¥å…·è°ƒç”¨ï¼ˆfunction callingï¼‰
3. ä½¿ç”¨AgentExecutorç®¡ç†æ‰§è¡Œæµç¨‹
4. å®Œå…¨å…¼å®¹LangChainç”Ÿæ€ç³»ç»Ÿ
"""

import asyncio
from typing import Dict, Any, List, Optional, AsyncIterator
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.tools import BaseTool
from langchain.agents import create_tool_calling_agent, AgentExecutor
try:
    from langchain_ollama import ChatOllama
except ImportError:
    from langchain_community.chat_models import ChatOllama

from ..utils.logger import get_logger
from ..tools.tool_service import get_tool_service, initialize_tool_service
from ..memory.memory_service import get_memory_service, initialize_memory_service
from ..config import config

logger = get_logger(__name__)


class AgentAgent:
    """
    åŸºäºLangChainåŸç”ŸAgentå®ç°
    
    ä½¿ç”¨LangChainæ ‡å‡†çš„create_tool_calling_agentå’ŒAgentExecutorï¼š
    1. æ”¯æŒåŸç”Ÿå·¥å…·è°ƒç”¨ï¼ˆfunction callingï¼‰
    2. ä½¿ç”¨AgentExecutorç®¡ç†æ‰§è¡Œæµç¨‹
    3. æ”¯æŒæµå¼è¾“å‡º
    4. å®Œå…¨å…¼å®¹LangChainç”Ÿæ€ç³»ç»Ÿ
    """
    
    def __init__(self, provider: str = "ollama", model: str = "qwen2.5:7b"):
        self.provider = provider
        self.model = model
        
        # LangChainç»„ä»¶
        self.llm = None
        self.tool_service = get_tool_service()
        self.memory_service = get_memory_service()
        
        # Agentç»„ä»¶
        self.agent = None
        self.agent_executor = None
        
        # çŠ¶æ€
        self.initialized = False
        
        logger.info(f"AgentAgent created for {provider}:{model}")

    async def _initialize_llm(self) -> bool:
        """åˆå§‹åŒ–LLM"""
        try:
            if self.provider == "ollama":
                self.llm = ChatOllama(
                    model=self.model,
                    temperature=0.7,
                    streaming=True,
                    base_url=config.OLLAMA_BASE_URL
                )
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")

            logger.info(f"LLM initialized: {self.provider}:{self.model}")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            return False

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–Agent"""
        try:
            # 1. åˆå§‹åŒ–LLM
            success = await self._initialize_llm()
            if not success:
                return False

            # 2. åˆå§‹åŒ–æœåŠ¡
            await initialize_memory_service()
            await initialize_tool_service(self.llm, self.provider, self.model)
            
            # 3. æ„å»ºAgent - ä½¿ç”¨LangChainæ ‡å‡†æ–¹å¼
            await self._build_agent()
            
            self.initialized = True
            logger.info("AgentAgent initialized successfully")
            return True
            
        except Exception as e:
            import traceback
            logger.error(f"Failed to initialize AgentAgent: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return False
    
    async def _build_agent(self):
        """æ„å»ºAgent - å®Œå…¨æŒ‰ç…§LangChainæ ‡å‡†å®ç°"""
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
        supports_tools = config.model_supports_tools(self.provider, self.model)
        
        if supports_tools and self.tool_service.get_tools():
            # ä½¿ç”¨åŸç”Ÿå·¥å…·è°ƒç”¨Agent
            await self._build_tool_calling_agent()
        else:
            # ä½¿ç”¨åŸºç¡€å¯¹è¯é“¾
            await self._build_conversation_chain()
    
    async def _build_tool_calling_agent(self):
        """æ„å»ºå·¥å…·è°ƒç”¨Agent - æŒ‰ç…§LangChainå®˜æ–¹å®ç°"""

        # è·å–LangChainå·¥å…·
        tools = self.tool_service.get_tools()
        logger.info(f"AgentAgent: Got {len(tools)} tools from tool service")

        # éªŒè¯å·¥å…·
        valid_tools = []
        for tool in tools:
            if hasattr(tool, 'name') and hasattr(tool, 'description'):
                valid_tools.append(tool)
                logger.debug(f"Valid tool: {tool.name}")
            else:
                logger.warning(f"Invalid tool: {tool}")

        logger.info(f"AgentAgent: Using {len(valid_tools)} valid tools")

        # åˆ›å»ºæç¤ºæ¨¡æ¿ - æŒ‰ç…§LangChainæ ‡å‡†
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant. Use tools when appropriate to help the user."),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

        # åˆ›å»ºAgent - ä½¿ç”¨LangChainçš„create_tool_calling_agent
        # æ³¨æ„ï¼šå¯¹äºä¸æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹ï¼ŒLangChainä¼šè‡ªåŠ¨ä½¿ç”¨æç¤ºè¯æ–¹å¼
        self.agent = create_tool_calling_agent(self.llm, valid_tools, prompt)

        # åˆ›å»ºAgentExecutor - LangChainæ ‡å‡†æ‰§è¡Œå™¨
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=valid_tools,
            verbose=True,
            return_intermediate_steps=True,
            handle_parsing_errors=True,
            max_iterations=5  # é™åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•°
        )
        
        logger.info("Tool calling agent built successfully")
    
    async def _build_conversation_chain(self):
        """æ„å»ºå¯¹è¯é“¾ - æŒ‰ç…§LangChain Runnableæ¥å£"""
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant."),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}")
        ])
        
        # æ„å»ºé“¾ - ä½¿ç”¨LangChainçš„Runnableæ¥å£
        from langchain_core.runnables import RunnablePassthrough
        from langchain_core.output_parsers import StrOutputParser
        
        self.chain = (
            RunnablePassthrough.assign(
                chat_history=lambda x: self._get_chat_history(x.get("session_id"))
            )
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        logger.info("Conversation chain built successfully")
    
    def _get_chat_history(self, session_id: str) -> List[BaseMessage]:
        """è·å–èŠå¤©å†å²"""
        if not session_id:
            return []
        
        try:
            return self.memory_service.get_chat_history_sync(session_id, limit=10)
        except Exception as e:
            logger.error(f"Failed to get chat history: {e}")
            return []
    
    async def chat(self, message: str, session_id: str = None, **kwargs) -> Dict[str, Any]:
        """èŠå¤©æ–¹æ³•"""
        try:
            if not self.initialized:
                return {"success": False, "error": "Agent not initialized"}
            
            # å‡†å¤‡è¾“å…¥
            input_data = {
                "input": message,
                "chat_history": self._get_chat_history(session_id)
            }
            
            # æ‰§è¡ŒAgentæˆ–Chain
            if self.agent_executor:
                # ä½¿ç”¨AgentExecutor
                result = await self.agent_executor.ainvoke(input_data)
                response_content = result.get("output", "")
                
                # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
                tool_calls = []
                intermediate_steps = result.get("intermediate_steps", [])
                for step in intermediate_steps:
                    if len(step) >= 2:
                        action, observation = step
                        tool_calls.append({
                            "tool": action.tool,
                            "input": action.tool_input,
                            "result": str(observation),
                            "success": True
                        })
            
            elif self.chain:
                # ä½¿ç”¨å¯¹è¯é“¾
                response_content = await self.chain.ainvoke(input_data)
                tool_calls = []
            
            else:
                return {"success": False, "error": "No agent or chain available"}
            
            # ä¿å­˜åˆ°å†…å­˜
            if session_id:
                await self.memory_service.add_message(
                    session_id, HumanMessage(content=message)
                )
                await self.memory_service.add_message(
                    session_id, AIMessage(content=response_content)
                )
            
            return {
                "success": True,
                "content": response_content,
                "tool_calls": tool_calls,
                "model_info": {
                    "provider": self.provider,
                    "model": self.model,
                    "type": "agent"
                }
            }
            
        except Exception as e:
            logger.error(f"Chat failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "content": f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}"
            }
    
    async def chat_stream(self, message: str, session_id: str = None, **kwargs) -> AsyncIterator[Dict[str, Any]]:
        """æµå¼èŠå¤©"""
        try:
            if not self.initialized:
                yield {"success": False, "error": "Agent not initialized", "done": True}
                return
            
            # å¯¹äºAgentæ¨¡å¼ï¼Œå…ˆæ‰§è¡Œå®Œæ•´å¯¹è¯ï¼Œç„¶åæµå¼è¿”å›
            response = await self.chat(message, session_id, **kwargs)
            
            if response.get("success"):
                content = response.get("content", "")
                
                # æ¨¡æ‹Ÿæµå¼è¾“å‡º
                words = content.split()
                for i, word in enumerate(words):
                    yield {
                        "success": True,
                        "content": word + " ",
                        "done": False
                    }
                    await asyncio.sleep(0.01)  # å°å»¶è¿Ÿæ¨¡æ‹Ÿæµå¼
                
                # è¾“å‡ºå·¥å…·è°ƒç”¨ä¿¡æ¯
                tool_calls = response.get("tool_calls", [])
                for call in tool_calls:
                    yield {
                        "success": True,
                        "content": f"\nğŸ”§ å·¥å…·è°ƒç”¨: {call['tool']} -> {call['result']}",
                        "done": False,
                        "tool_call": call
                    }
            
            yield {"success": True, "content": "", "done": True}
            
        except Exception as e:
            logger.error(f"Stream chat failed: {e}")
            yield {
                "success": False,
                "error": str(e),
                "content": f"æµå¼å¤„ç†å‡ºé”™: {str(e)}",
                "done": True
            }
    
    async def get_info(self) -> Dict[str, Any]:
        """è·å–Agentä¿¡æ¯"""
        return {
            "type": "agent",
            "provider": self.provider,
            "model": self.model,
            "initialized": self.initialized,
            "supports_tools": True,
            "tools_count": len(self.tool_service.get_tools()),
            "memory_enabled": True
        }
    
    async def clear_memory(self, session_id: str) -> bool:
        """æ¸…é™¤å†…å­˜"""
        return await self.memory_service.clear_session(session_id)

    async def switch_model(self, new_model: str) -> bool:
        """åˆ‡æ¢åº•å±‚æ¨¡å‹"""
        try:
            old_model = self.model
            self.model = new_model

            # é‡æ–°åˆå§‹åŒ–LLM
            success = await self._initialize_llm()
            if success:
                # é‡æ–°æ„å»ºagent
                await self._build_agent()
                logger.info(f"Successfully switched from {old_model} to {new_model}")
                return True
            else:
                # æ¢å¤åŸæ¨¡å‹
                self.model = old_model
                logger.error(f"Failed to switch to {new_model}, reverted to {old_model}")
                return False

        except Exception as e:
            logger.error(f"Error switching model: {e}")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "current_model": self.model,
            "provider": self.provider,
            "agent_type": "agent",
            "initialized": self.initialized
        }
