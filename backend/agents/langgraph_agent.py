"""
LangGraphæ–¹å¼çš„Agentå®ç°
ä½¿ç”¨LangGraphå®ç°æ›´å¤æ‚çš„å·¥ä½œæµå’ŒçŠ¶æ€ç®¡ç†

æ ¸å¿ƒç‰¹ç‚¹ï¼š
1. ä½¿ç”¨LangGraphæ„å»ºçŠ¶æ€å›¾
2. æ”¯æŒå¤æ‚çš„å·¥ä½œæµæ§åˆ¶
3. çŠ¶æ€ç®¡ç†å’ŒæŒä¹…åŒ–
4. æ”¯æŒæ¡ä»¶åˆ†æ”¯å’Œå¾ªç¯
"""

from typing import Dict, Any, List, Optional, AsyncIterator, TypedDict, Annotated
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
try:
    from langchain_ollama import ChatOllama
except ImportError:
    from langchain_community.chat_models import ChatOllama

try:
    from langgraph.graph import StateGraph, END
    from langgraph.prebuilt import ToolNode
    from langgraph.checkpoint.memory import MemorySaver
    # å°è¯•å¯¼å…¥add_messagesï¼Œå¦‚æœå¤±è´¥åˆ™å®šä¹‰ä¸€ä¸ªç®€å•ç‰ˆæœ¬
    try:
        from langgraph.graph.message import add_messages
    except ImportError:
        def add_messages(left, right):
            """ç®€å•çš„æ¶ˆæ¯åˆå¹¶å‡½æ•°"""
            return left + right
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    # åˆ›å»ºæ¨¡æ‹Ÿç±»ä»¥é¿å…å¯¼å…¥é”™è¯¯
    class StateGraph:
        def __init__(self, *args, **kwargs):
            pass
        def add_node(self, *args, **kwargs):
            pass
        def add_edge(self, *args, **kwargs):
            pass
        def add_conditional_edges(self, *args, **kwargs):
            pass
        def set_entry_point(self, *args, **kwargs):
            pass
        def compile(self, *args, **kwargs):
            return self

    class ToolNode:
        def __init__(self, *args, **kwargs):
            pass

    class MemorySaver:
        def __init__(self, *args, **kwargs):
            pass

    END = "END"

    def add_messages(left, right):
        return left + right

from ..utils.logger import get_logger
from ..tools.tool_service import get_tool_service, initialize_tool_service
from ..memory.memory_service import get_memory_service, initialize_memory_service
from ..config import config

logger = get_logger(__name__)


class AgentState(TypedDict):
    """AgentçŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[BaseMessage], add_messages]
    session_id: str
    tool_calls: List[Dict[str, Any]]
    iteration_count: int


class LangGraphAgent:
    """
    åŸºäºLangGraphçš„Agentå®ç°
    
    ä½¿ç”¨LangGraphæ„å»ºå¤æ‚çš„çŠ¶æ€å›¾ï¼š
    1. æ”¯æŒå¤æ‚å·¥ä½œæµæ§åˆ¶
    2. çŠ¶æ€ç®¡ç†å’ŒæŒä¹…åŒ–
    3. æ¡ä»¶åˆ†æ”¯å’Œå¾ªç¯
    4. å·¥å…·è°ƒç”¨ç®¡ç†
    """
    
    def __init__(self, provider: str = "ollama", model: str = "qwen2.5:7b"):
        if not LANGGRAPH_AVAILABLE:
            raise ImportError("LangGraph not available. Please install with: pip install langgraph")
        
        self.provider = provider
        self.model = model
        
        # æ ¸å¿ƒç»„ä»¶
        self.llm = None
        self.tool_service = get_tool_service()
        self.memory_service = get_memory_service()
        
        # LangGraphç»„ä»¶
        self.graph = None
        self.checkpointer = None
        
        # çŠ¶æ€
        self.initialized = False
        
        logger.info(f"LangGraphAgent created for {provider}:{model}")

    async def _initialize_llm(self) -> bool:
        """åˆå§‹åŒ–LLM"""
        try:
            if self.provider == "ollama":
                self.llm = ChatOllama(
                    model=self.model,
                    temperature=0.7,
                    streaming=True,
                    base_url=config.OLLAMA_BASE_URL
                )
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")

            logger.info(f"LLM initialized: {self.provider}:{self.model}")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            return False

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–Agent"""
        try:
            # æ£€æŸ¥LangGraphæ˜¯å¦å¯ç”¨
            if not LANGGRAPH_AVAILABLE:
                logger.error("LangGraph is not available. Please install langgraph: pip install langgraph")
                return False

            # 1. åˆå§‹åŒ–LLM
            success = await self._initialize_llm()
            if not success:
                return False

            # 2. åˆå§‹åŒ–æœåŠ¡
            await initialize_memory_service()
            await initialize_tool_service(self.llm, self.provider, self.model)
            
            # 3. åŠ è½½å·¥å…·
            await self._load_tools()
            
            # 4. æ„å»ºLangGraph
            await self._build_graph()
            
            self.initialized = True
            logger.info("LangGraphAgent initialized successfully")
            return True
            
        except Exception as e:
            import traceback
            logger.error(f"Failed to initialize LangGraphAgent: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return False
    
    async def _load_tools(self):
        """åŠ è½½å·¥å…·"""
        # ç›´æ¥ä½¿ç”¨å·¥å…·æœåŠ¡ï¼Œæ— éœ€å•ç‹¬åŠ è½½
        tools = self.tool_service.get_tools()
        logger.info(f"Using {len(tools)} tools from tool service")
    
    async def _build_graph(self):
        """æ„å»ºLangGraphçŠ¶æ€å›¾"""
        
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(AgentState)
        
        # è·å–å·¥å…·
        tools = self.tool_service.get_tools()
        
        # ç»‘å®šå·¥å…·åˆ°LLM
        try:
            if tools and hasattr(self.llm, 'bind_tools'):
                self.llm_with_tools = self.llm.bind_tools(tools)
                logger.info("Successfully bound tools to LLM for LangGraph")
            else:
                self.llm_with_tools = self.llm
                logger.warning("LLM does not support tool binding, using without tools")
                tools = []
        except NotImplementedError:
            logger.warning("LLM does not support bind_tools for LangGraph")
            self.llm_with_tools = self.llm
            tools = []
        except Exception as e:
            logger.error(f"Failed to bind tools in LangGraph: {e}")
            self.llm_with_tools = self.llm
            tools = []
            
            # åˆ›å»ºå·¥å…·èŠ‚ç‚¹
            tool_node = ToolNode(tools)
            
            # æ·»åŠ èŠ‚ç‚¹
            workflow.add_node("agent", self._call_model)
            workflow.add_node("tools", tool_node)
            
            # è®¾ç½®å…¥å£ç‚¹
            workflow.set_entry_point("agent")
            
            # æ·»åŠ æ¡ä»¶è¾¹
            workflow.add_conditional_edges(
                "agent",
                self._should_continue,
                {
                    "continue": "tools",
                    "end": END,
                }
            )
            
            # å·¥å…·æ‰§è¡Œåå›åˆ°agent
            workflow.add_edge("tools", "agent")
            
        else:
            # æ²¡æœ‰å·¥å…·çš„ç®€å•å¯¹è¯æ¨¡å¼
            workflow.add_node("agent", self._call_model)
            workflow.set_entry_point("agent")
            workflow.add_edge("agent", END)
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ä¿å­˜å™¨
        self.checkpointer = MemorySaver()
        
        # ç¼–è¯‘å›¾
        self.graph = workflow.compile(checkpointer=self.checkpointer)
        
        logger.info("LangGraph built successfully")
    
    def _call_model(self, state: AgentState) -> Dict[str, Any]:
        """è°ƒç”¨æ¨¡å‹"""
        messages = state["messages"]

        # è°ƒç”¨æ¨¡å‹ - ç›´æ¥ä¼ é€’æ¶ˆæ¯åˆ—è¡¨
        if hasattr(self, 'llm_with_tools'):
            response = self.llm_with_tools.invoke(messages)
        else:
            response = self.llm.invoke(messages)

        return {"messages": [response]}
    
    def _should_continue(self, state: AgentState) -> str:
        """å†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œå·¥å…·"""
        messages = state["messages"]
        last_message = messages[-1]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            return "continue"
        else:
            return "end"
    
    async def chat(self, message: str, session_id: str = None, **kwargs) -> Dict[str, Any]:
        """èŠå¤©æ–¹æ³•"""
        try:
            if not self.initialized:
                return {"success": False, "error": "Agent not initialized"}
            
            # è·å–å†å²æ¶ˆæ¯
            chat_history = []
            if session_id and self.memory_service:
                history = await self.memory_service.get_chat_history(session_id, limit=10)
                chat_history = history

            # å‡†å¤‡åˆå§‹çŠ¶æ€
            all_messages = chat_history + [HumanMessage(content=message)]
            initial_state = {
                "messages": all_messages,
                "session_id": session_id or "default",
                "tool_calls": [],
                "iteration_count": 0
            }
            
            # é…ç½®
            config_dict = {"configurable": {"thread_id": session_id or "default"}}
            
            # æ‰§è¡Œå›¾
            result = await self.graph.ainvoke(initial_state, config=config_dict)
            
            # æå–å“åº”
            messages = result["messages"]
            last_message = messages[-1] if messages else None
            
            if last_message:
                response_content = last_message.content
                
                # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
                tool_calls = []
                if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                    for tool_call in last_message.tool_calls:
                        tool_calls.append({
                            "tool": tool_call.get("name", "unknown"),
                            "input": tool_call.get("args", {}),
                            "result": "Tool executed",
                            "success": True
                        })
            else:
                response_content = "No response generated"
                tool_calls = []
            
            # ä¿å­˜åˆ°å†…å­˜
            if session_id and self.memory_service:
                await self.memory_service.add_message(
                    session_id, HumanMessage(content=message)
                )
                await self.memory_service.add_message(
                    session_id, AIMessage(content=response_content)
                )
            
            return {
                "success": True,
                "content": response_content,
                "tool_calls": tool_calls,
                "model_info": {
                    "provider": self.provider,
                    "model": self.model,
                    "type": "langgraph"
                }
            }
            
        except Exception as e:
            logger.error(f"Chat failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "content": f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}"
            }
    
    async def chat_stream(self, message: str, session_id: str = None, **kwargs) -> AsyncIterator[Dict[str, Any]]:
        """æµå¼èŠå¤©"""
        try:
            if not self.initialized:
                yield {"success": False, "error": "Agent not initialized", "done": True}
                return
            
            # å‡†å¤‡åˆå§‹çŠ¶æ€
            initial_state = {
                "messages": [HumanMessage(content=message)],
                "session_id": session_id or "default",
                "tool_calls": [],
                "iteration_count": 0
            }
            
            # é…ç½®
            config_dict = {"configurable": {"thread_id": session_id or "default"}}
            
            # æµå¼æ‰§è¡Œ
            full_response = ""
            async for chunk in self.graph.astream(initial_state, config=config_dict):
                for node_name, node_output in chunk.items():
                    if "messages" in node_output:
                        messages = node_output["messages"]
                        if messages:
                            last_message = messages[-1]
                            if hasattr(last_message, 'content') and last_message.content:
                                content = last_message.content
                                if content != full_response:
                                    new_content = content[len(full_response):]
                                    full_response = content
                                    yield {
                                        "success": True,
                                        "content": new_content,
                                        "done": False
                                    }
                            
                            # æ£€æŸ¥å·¥å…·è°ƒç”¨
                            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                                for tool_call in last_message.tool_calls:
                                    yield {
                                        "success": True,
                                        "content": f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tool_call.get('name', 'unknown')}",
                                        "done": False,
                                        "tool_call": {
                                            "tool": tool_call.get('name', 'unknown'),
                                            "input": tool_call.get('args', {})
                                        }
                                    }
            
            yield {"success": True, "content": "", "done": True}
            
            # ä¿å­˜åˆ°å†…å­˜
            if session_id and self.memory_manager:
                await self.memory_manager.add_message(
                    session_id, HumanMessage(content=message)
                )
                await self.memory_manager.add_message(
                    session_id, AIMessage(content=full_response)
                )
            
        except Exception as e:
            logger.error(f"Stream chat failed: {e}")
            yield {
                "success": False,
                "error": str(e),
                "content": f"æµå¼å¤„ç†å‡ºé”™: {str(e)}",
                "done": True
            }
    
    async def get_info(self) -> Dict[str, Any]:
        """è·å–Agentä¿¡æ¯"""
        return {
            "type": "langgraph",
            "provider": self.provider,
            "model": self.model,
            "initialized": self.initialized,
            "supports_tools": True,
            "tools_count": len(self.tool_manager.get_tools()) if self.tool_manager else 0,
            "memory_enabled": bool(self.memory_manager),
            "langgraph_available": LANGGRAPH_AVAILABLE
        }
    
    async def clear_memory(self, session_id: str) -> bool:
        """æ¸…é™¤å†…å­˜"""
        if self.memory_manager:
            return await self.memory_manager.clear_session(session_id)
        return False

    async def switch_model(self, new_model: str) -> bool:
        """åˆ‡æ¢åº•å±‚æ¨¡å‹"""
        try:
            old_model = self.model
            self.model = new_model

            # é‡æ–°åˆå§‹åŒ–LLM
            success = await self._initialize_llm()
            if success:
                # é‡æ–°æ„å»ºgraph
                await self._build_graph()
                logger.info(f"Successfully switched from {old_model} to {new_model}")
                return True
            else:
                # æ¢å¤åŸæ¨¡å‹
                self.model = old_model
                logger.error(f"Failed to switch to {new_model}, reverted to {old_model}")
                return False

        except Exception as e:
            logger.error(f"Error switching model: {e}")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "current_model": self.model,
            "provider": self.provider,
            "agent_type": "langgraph",
            "initialized": self.initialized
        }
