"""
OpenWebUIæ¨¡å‹æä¾›è€…
ä¸ºOpenWebUIæä¾›Agentæ¨¡å¼å’Œæ¨¡å‹åˆ†ç¦»çš„è‡ªå®šä¹‰æ¨¡å‹
"""

from fastapi import APIRouter, HTTPException
from typing import Dict, List, Any
import logging
import httpx

logger = logging.getLogger(__name__)

router = APIRouter()


async def get_ollama_models():
    """è·å–Ollamaå¯ç”¨æ¨¡å‹"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get("http://host.docker.internal:11434/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return data.get("models", [])
    except Exception as e:
        logger.warning(f"Failed to get Ollama models: {e}")
    
    # è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
    return [
        {"name": "qwen2.5:7b", "size": 4000000000},
        {"name": "qwen2.5:14b", "size": 8000000000},
        {"name": "llama3.1:8b", "size": 5000000000},
        {"name": "mistral:7b", "size": 4000000000}
    ]


@router.get("/api/models")
async def get_openwebui_models():
    """
    ä¸ºOpenWebUIæä¾›æ¨¡å‹åˆ—è¡¨
    åŒ…å«Agentæ¨¡å¼å’Œæ¨¡å‹çš„ç»„åˆ
    """
    try:
        models = []
        
        # è·å–Ollamaæ¨¡å‹
        ollama_models = await get_ollama_models()
        
        # Agentæ¨¡å¼å®šä¹‰
        agent_modes = {
            "chain": {
                "name": "Chain Agent",
                "description": "åŸºäºRunnableæ¥å£çš„ç®€å•Agentï¼Œé€‚åˆæ—¥å¸¸å¯¹è¯",
                "icon": "ğŸ”—"
            },
            "agent": {
                "name": "Tool Agent", 
                "description": "æ”¯æŒå·¥å…·è°ƒç”¨çš„æ™ºèƒ½Agentï¼Œé€‚åˆå¤æ‚ä»»åŠ¡",
                "icon": "ğŸ› ï¸"
            },
            "langgraph": {
                "name": "Graph Agent",
                "description": "åŸºäºçŠ¶æ€å›¾çš„é«˜çº§Agentï¼Œé€‚åˆå¤æ‚å·¥ä½œæµ",
                "icon": "ğŸ•¸ï¸"
            }
        }
        
        # ä¸ºæ¯ä¸ªAgentæ¨¡å¼å’Œæ¨¡å‹ç»„åˆåˆ›å»ºæ¨¡å‹æ¡ç›®
        for mode_id, mode_info in agent_modes.items():
            for ollama_model in ollama_models:
                model_name = ollama_model.get("name", "")
                model_size = ollama_model.get("size", 0)
                
                # è®¡ç®—æ¨¡å‹å¤§å°
                size_gb = f"{model_size / 1000000000:.1f}GB" if model_size > 0 else ""
                
                # æ„å»ºæ¨¡å‹IDå’Œæ˜¾ç¤ºåç§°
                model_id = f"langchain-{mode_id}-{model_name.replace(':', '-')}"
                display_name = f"{mode_info['icon']} {mode_info['name']} + {model_name}"
                if size_gb:
                    display_name += f" ({size_gb})"
                
                models.append({
                    "id": model_id,
                    "name": display_name,
                    "object": "model",
                    "created": 1699000000,
                    "owned_by": "langchain",
                    "permission": [],
                    "root": model_id,
                    "parent": None,
                    "description": f"{mode_info['description']} - ä½¿ç”¨ {model_name} æ¨¡å‹",
                    "metadata": {
                        "agent_mode": mode_id,
                        "ollama_model": model_name,
                        "type": "agent_combo"
                    }
                })
        
        # æ·»åŠ Agenté…ç½®å™¨
        models.append({
            "id": "langchain-configurator",
            "name": "ğŸ”§ Agenté…ç½®å™¨",
            "object": "model",
            "created": 1699000000,
            "owned_by": "langchain",
            "permission": [],
            "root": "langchain-configurator",
            "parent": None,
            "description": "é…ç½®å’Œç®¡ç†Agentæ¨¡å¼ä¸æ¨¡å‹",
            "metadata": {
                "agent_mode": "config",
                "ollama_model": "none",
                "type": "configurator"
            }
        })
        
        return {"data": models}
        
    except Exception as e:
        logger.error(f"Failed to get OpenWebUI models: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/api/chat/completions")
async def handle_chat_completions(request: Dict[str, Any]):
    """
    å¤„ç†OpenWebUIçš„èŠå¤©å®Œæˆè¯·æ±‚
    æ ¹æ®æ¨¡å‹IDè·¯ç”±åˆ°ç›¸åº”çš„Agent
    """
    try:
        model_id = request.get("model", "")
        messages = request.get("messages", [])
        
        # å¤„ç†é…ç½®å™¨è¯·æ±‚
        if model_id == "langchain-configurator":
            return await handle_configurator_request(messages, request)
        
        # å¤„ç†Agentç»„åˆè¯·æ±‚
        if model_id.startswith("langchain-") and "-" in model_id:
            return await handle_agent_combo_request(model_id, request)
        
        # é»˜è®¤å¤„ç†
        return {
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1699000000,
            "model": model_id,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": f"æœªçŸ¥æ¨¡å‹: {model_id}"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }
        
    except Exception as e:
        logger.error(f"Error handling chat completion: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def handle_configurator_request(messages: List[Dict], request: Dict[str, Any]):
    """å¤„ç†é…ç½®å™¨è¯·æ±‚"""
    try:
        if not messages:
            content = get_configurator_help()
        else:
            last_message = messages[-1].get("content", "").lower()
            
            if "åˆ—å‡ºæ¨¡å¼" in last_message or "list modes" in last_message:
                content = await get_agent_modes_info()
            elif "åˆ—å‡ºæ¨¡å‹" in last_message or "list models" in last_message:
                content = await get_ollama_models_info()
            elif "é…ç½®" in last_message:
                content = await handle_configuration_command(last_message)
            elif "çŠ¶æ€" in last_message or "status" in last_message:
                content = await get_agent_status()
            else:
                content = get_configurator_help()
        
        return {
            "id": "chatcmpl-config",
            "object": "chat.completion",
            "created": 1699000000,
            "model": "langchain-configurator",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(str(messages)),
                "completion_tokens": len(content),
                "total_tokens": len(str(messages)) + len(content)
            }
        }
        
    except Exception as e:
        logger.error(f"Error handling configurator request: {e}")
        return {
            "id": "chatcmpl-error",
            "object": "chat.completion",
            "created": 1699000000,
            "model": "langchain-configurator",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": f"é…ç½®å™¨é”™è¯¯: {str(e)}"
                },
                "finish_reason": "stop"
            }],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }


async def handle_agent_combo_request(model_id: str, request: Dict[str, Any]):
    """å¤„ç†Agentç»„åˆè¯·æ±‚"""
    try:
        # è§£ææ¨¡å‹ID
        parts = model_id.replace("langchain-", "").split("-", 1)
        if len(parts) < 2:
            raise ValueError(f"Invalid model ID: {model_id}")
        
        agent_mode = parts[0]
        ollama_model = parts[1].replace("-", ":")
        
        # é…ç½®Agent
        await configure_agent_backend(agent_mode, ollama_model)
        
        # è½¬å‘è¯·æ±‚åˆ°LangChainåç«¯
        backend_model = f"langchain-{agent_mode}-mode"
        backend_request = {**request, "model": backend_model}
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://langchain-backend:8000/v1/chat/completions",
                json=backend_request,
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                raise HTTPException(status_code=response.status_code, detail=response.text)
                
    except Exception as e:
        logger.error(f"Error handling agent combo request: {e}")
        return {
            "id": "chatcmpl-error",
            "object": "chat.completion",
            "created": 1699000000,
            "model": model_id,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": f"Agentè¯·æ±‚é”™è¯¯: {str(e)}"
                },
                "finish_reason": "stop"
            }],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }


async def configure_agent_backend(mode: str, model: str):
    """é…ç½®Agentåç«¯"""
    try:
        payload = {"mode": mode, "model": model}
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://langchain-backend:8000/v1/agent/configure",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                logger.info(f"Successfully configured {mode} agent with {model} model")
            else:
                logger.warning(f"Failed to configure agent: {response.status_code}")
                
    except Exception as e:
        logger.error(f"Error configuring agent: {e}")


def get_configurator_help():
    """è·å–é…ç½®å™¨å¸®åŠ©ä¿¡æ¯"""
    return """
ğŸ¯ LangChain Agenté…ç½®å™¨

**ä½¿ç”¨æ–¹æ³•:**
1. åœ¨æ¨¡å‹åˆ—è¡¨ä¸­é€‰æ‹©Agentæ¨¡å¼å’Œæ¨¡å‹çš„ç»„åˆ
2. ç³»ç»Ÿä¼šè‡ªåŠ¨é…ç½®ç›¸åº”çš„Agent
3. å¼€å§‹å¯¹è¯ï¼

**å¯ç”¨å‘½ä»¤:**
â€¢ "åˆ—å‡ºæ¨¡å¼" - æŸ¥çœ‹æ‰€æœ‰Agentæ¨¡å¼
â€¢ "åˆ—å‡ºæ¨¡å‹" - æŸ¥çœ‹æ‰€æœ‰Ollamaæ¨¡å‹
â€¢ "é…ç½®chain Agentä½¿ç”¨qwen2.5:7bæ¨¡å‹" - æ‰‹åŠ¨é…ç½®
â€¢ "è·å–çŠ¶æ€" - æŸ¥çœ‹å½“å‰é…ç½®

**Agentæ¨¡å¼:**
ğŸ”— **Chain Agent** - é€‚åˆæ—¥å¸¸å¯¹è¯å’Œç®€å•ä»»åŠ¡
ğŸ› ï¸ **Tool Agent** - é€‚åˆå·¥å…·è°ƒç”¨å’Œå¤æ‚ä»»åŠ¡  
ğŸ•¸ï¸ **Graph Agent** - é€‚åˆå¤æ‚å·¥ä½œæµå’ŒçŠ¶æ€ç®¡ç†

**å¿«é€Ÿå¼€å§‹:**
ç›´æ¥ä»æ¨¡å‹åˆ—è¡¨ä¸­é€‰æ‹©æƒ³è¦çš„Agentæ¨¡å¼å’Œæ¨¡å‹ç»„åˆå³å¯ï¼
"""


async def get_agent_modes_info():
    """è·å–Agentæ¨¡å¼ä¿¡æ¯"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get("http://langchain-backend:8000/v1/agent/modes", timeout=10)
            if response.status_code == 200:
                data = response.json()
                modes = data.get("modes", {})
                
                result = "ğŸ¤– å¯ç”¨çš„Agentæ¨¡å¼:\n\n"
                for mode_id, mode_info in modes.items():
                    result += f"**{mode_info['name']}** ({mode_id})\n"
                    result += f"æè¿°: {mode_info['description']}\n"
                    result += f"ç‰¹æ€§: {', '.join(mode_info['features'])}\n\n"
                
                return result
            else:
                return f"âŒ è·å–Agentæ¨¡å¼å¤±è´¥: {response.status_code}"
    except Exception as e:
        return f"âŒ è¯·æ±‚å¼‚å¸¸: {e}"


async def get_ollama_models_info():
    """è·å–Ollamaæ¨¡å‹ä¿¡æ¯"""
    try:
        models = await get_ollama_models()
        result = "ğŸ§  å¯ç”¨çš„Ollamaæ¨¡å‹:\n\n"
        for model in models:
            name = model.get("name", "")
            size = model.get("size", 0)
            size_gb = f"{size / 1000000000:.1f}GB" if size > 0 else "æœªçŸ¥å¤§å°"
            result += f"â€¢ **{name}** ({size_gb})\n"
        return result
    except Exception as e:
        return f"âŒ è·å–æ¨¡å‹å¤±è´¥: {e}"


async def handle_configuration_command(command: str):
    """å¤„ç†é…ç½®å‘½ä»¤"""
    # ç®€å•çš„é…ç½®è§£æ
    if "chain" in command:
        mode = "chain"
    elif "agent" in command:
        mode = "agent"
    elif "langgraph" in command or "graph" in command:
        mode = "langgraph"
    else:
        return "âŒ è¯·æŒ‡å®šAgentæ¨¡å¼ (chain, agent, langgraph)"
    
    # æå–æ¨¡å‹åç§°
    model = None
    if "qwen2.5:7b" in command:
        model = "qwen2.5:7b"
    elif "qwen2.5:14b" in command:
        model = "qwen2.5:14b"
    elif "llama3.1:8b" in command:
        model = "llama3.1:8b"
    elif "mistral:7b" in command:
        model = "mistral:7b"
    
    if not model:
        return f"âŒ è¯·æŒ‡å®šæ¨¡å‹åç§°ï¼Œä¾‹å¦‚: é…ç½®{mode} Agentä½¿ç”¨qwen2.5:7bæ¨¡å‹"
    
    try:
        await configure_agent_backend(mode, model)
        return f"âœ… é…ç½®æˆåŠŸï¼{mode} Agent ç°åœ¨ä½¿ç”¨ {model} æ¨¡å‹"
    except Exception as e:
        return f"âŒ é…ç½®å¤±è´¥: {e}"


async def get_agent_status():
    """è·å–AgentçŠ¶æ€"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get("http://langchain-backend:8000/v1/agent/current-config", timeout=10)
            if response.status_code == 200:
                data = response.json()
                config = data.get("current_config", {})
                
                result = "âš™ï¸ å½“å‰Agenté…ç½®:\n\n"
                for agent_type, agent_config in config.items():
                    mode = agent_config.get("mode", "")
                    model = agent_config.get("model", "")
                    status = agent_config.get("status", "")
                    result += f"**{agent_type}**: {mode} + {model} ({status})\n"
                
                return result
            else:
                return f"âŒ è·å–çŠ¶æ€å¤±è´¥: {response.status_code}"
    except Exception as e:
        return f"âŒ è¯·æ±‚å¼‚å¸¸: {e}"
