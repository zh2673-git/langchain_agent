"""
LangChain Agent Pipe for OpenWebUI
åˆ›å»ºAgentæ¨¡å¼å’Œæ¨¡å‹åˆ†ç¦»çš„è‡ªå®šä¹‰æ¨¡å‹é€‰æ‹©å™¨
"""

from pydantic import BaseModel, Field
import requests
import json
from typing import List, Dict, Any


class Pipe:
    class Valves(BaseModel):
        BACKEND_URL: str = Field(
            default="http://langchain-backend:8000",
            description="LangChainåç«¯APIåœ°å€"
        )
        OLLAMA_URL: str = Field(
            default="http://host.docker.internal:11434",
            description="Ollama APIåœ°å€"
        )
        ENABLE_AGENT_MODES: bool = Field(
            default=True,
            description="å¯ç”¨Agentæ¨¡å¼é€‰æ‹©"
        )
        SHOW_MODEL_INFO: bool = Field(
            default=True,
            description="åœ¨æ¨¡å‹åç§°ä¸­æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯"
        )

    def __init__(self):
        self.valves = self.Valves()
        self.agent_modes = {
            "chain": {
                "name": "Chain Agent",
                "description": "åŸºäºRunnableæ¥å£çš„ç®€å•Agentï¼Œé€‚åˆæ—¥å¸¸å¯¹è¯",
                "icon": "ğŸ”—",
                "features": ["å¿«é€Ÿå“åº”", "ç®€å•å¯¹è¯", "åŸºç¡€å·¥å…·è°ƒç”¨"]
            },
            "agent": {
                "name": "Tool Agent", 
                "description": "æ”¯æŒå·¥å…·è°ƒç”¨çš„æ™ºèƒ½Agentï¼Œé€‚åˆå¤æ‚ä»»åŠ¡",
                "icon": "ğŸ› ï¸",
                "features": ["å·¥å…·è°ƒç”¨", "æ¨ç†èƒ½åŠ›", "å¤šæ­¥éª¤ä»»åŠ¡"]
            },
            "langgraph": {
                "name": "Graph Agent",
                "description": "åŸºäºçŠ¶æ€å›¾çš„é«˜çº§Agentï¼Œé€‚åˆå¤æ‚å·¥ä½œæµ",
                "icon": "ğŸ•¸ï¸",
                "features": ["çŠ¶æ€ç®¡ç†", "å¤æ‚å·¥ä½œæµ", "æ¡ä»¶åˆ†æ”¯"]
            }
        }

    def get_ollama_models(self) -> List[Dict]:
        """è·å–Ollamaå¯ç”¨æ¨¡å‹"""
        try:
            response = requests.get(f"{self.valves.OLLAMA_URL}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return data.get("models", [])
        except Exception as e:
            print(f"Failed to get Ollama models: {e}")
        
        # è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
        return [
            {"name": "qwen2.5:7b", "size": 4000000000},
            {"name": "qwen2.5:14b", "size": 8000000000},
            {"name": "llama3.1:8b", "size": 5000000000},
            {"name": "mistral:7b", "size": 4000000000}
        ]

    def pipes(self) -> List[Dict]:
        """è¿”å›å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ˆAgentæ¨¡å¼ + æ¨¡å‹ç»„åˆï¼‰"""
        if not self.valves.ENABLE_AGENT_MODES:
            return []

        models = []
        ollama_models = self.get_ollama_models()
        
        # ä¸ºæ¯ä¸ªAgentæ¨¡å¼åˆ›å»ºæ¨¡å‹é€‰é¡¹
        for mode_id, mode_info in self.agent_modes.items():
            for ollama_model in ollama_models:
                model_name = ollama_model.get("name", "")
                model_size = ollama_model.get("size", 0)
                
                # è®¡ç®—æ¨¡å‹å¤§å°
                size_gb = f"{model_size / 1000000000:.1f}GB" if model_size > 0 else ""
                
                # æ„å»ºæ¨¡å‹IDå’Œæ˜¾ç¤ºåç§°
                model_id = f"langchain-{mode_id}-{model_name.replace(':', '-')}"
                
                if self.valves.SHOW_MODEL_INFO:
                    display_name = f"{mode_info['icon']} {mode_info['name']} + {model_name}"
                    if size_gb:
                        display_name += f" ({size_gb})"
                else:
                    display_name = f"{mode_info['name']} ({model_name})"
                
                models.append({
                    "id": model_id,
                    "name": display_name,
                    "description": f"{mode_info['description']} - ä½¿ç”¨ {model_name} æ¨¡å‹",
                    "metadata": {
                        "agent_mode": mode_id,
                        "ollama_model": model_name,
                        "features": mode_info["features"]
                    }
                })
        
        # æ·»åŠ æ¨¡å¼é€‰æ‹©å™¨ï¼ˆç”¨äºé…ç½®ï¼‰
        models.append({
            "id": "langchain-config",
            "name": "ğŸ”§ Agenté…ç½®å™¨",
            "description": "é…ç½®å’Œç®¡ç†Agentæ¨¡å¼ä¸æ¨¡å‹",
            "metadata": {
                "agent_mode": "config",
                "ollama_model": "none"
            }
        })
        
        return models

    async def pipe(self, body: dict, __user__: dict = None) -> str:
        """å¤„ç†è¯·æ±‚å¹¶è½¬å‘åˆ°ç›¸åº”çš„Agent"""
        try:
            model_id = body.get("model", "")
            messages = body.get("messages", [])
            
            # è§£ææ¨¡å‹ID
            if model_id == "langchain-config":
                return await self.handle_config_request(messages)
            
            if not model_id.startswith("langchain-"):
                return "é”™è¯¯ï¼šæ— æ•ˆçš„æ¨¡å‹ID"
            
            # æå–Agentæ¨¡å¼å’ŒOllamaæ¨¡å‹
            parts = model_id.replace("langchain-", "").split("-", 1)
            if len(parts) < 2:
                return "é”™è¯¯ï¼šæ— æ³•è§£ææ¨¡å‹ID"
            
            agent_mode = parts[0]
            ollama_model = parts[1].replace("-", ":")
            
            # é…ç½®Agent
            await self.configure_agent(agent_mode, ollama_model)
            
            # è½¬å‘è¯·æ±‚åˆ°LangChainåç«¯
            backend_model = f"langchain-{agent_mode}-mode"
            payload = {**body, "model": backend_model}
            
            response = requests.post(
                f"{self.valves.BACKEND_URL}/v1/chat/completions",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                return content
            else:
                return f"é”™è¯¯ï¼šåç«¯è¯·æ±‚å¤±è´¥ ({response.status_code})"
                
        except Exception as e:
            return f"é”™è¯¯ï¼š{str(e)}"

    async def configure_agent(self, mode: str, model: str):
        """é…ç½®Agentæ¨¡å¼å’Œæ¨¡å‹"""
        try:
            payload = {
                "mode": mode,
                "model": model
            }
            
            response = requests.post(
                f"{self.valves.BACKEND_URL}/v1/agent/configure",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                print(f"Successfully configured {mode} agent with {model} model")
            else:
                print(f"Failed to configure agent: {response.status_code}")
                
        except Exception as e:
            print(f"Error configuring agent: {e}")

    async def handle_config_request(self, messages: List[Dict]) -> str:
        """å¤„ç†é…ç½®è¯·æ±‚"""
        if not messages:
            return self.get_config_help()
        
        last_message = messages[-1].get("content", "").lower()
        
        if "åˆ—å‡º" in last_message or "list" in last_message:
            if "æ¨¡å¼" in last_message or "mode" in last_message:
                return self.list_agent_modes()
            elif "æ¨¡å‹" in last_message or "model" in last_message:
                return self.list_ollama_models()
        
        elif "é…ç½®" in last_message or "config" in last_message:
            return self.get_config_instructions()
        
        elif "å¸®åŠ©" in last_message or "help" in last_message:
            return self.get_config_help()
        
        return self.get_config_help()

    def list_agent_modes(self) -> str:
        """åˆ—å‡ºAgentæ¨¡å¼"""
        result = "ğŸ¤– å¯ç”¨çš„Agentæ¨¡å¼:\n\n"
        for mode_id, mode_info in self.agent_modes.items():
            result += f"**{mode_info['icon']} {mode_info['name']}** ({mode_id})\n"
            result += f"æè¿°: {mode_info['description']}\n"
            result += f"ç‰¹æ€§: {', '.join(mode_info['features'])}\n\n"
        return result

    def list_ollama_models(self) -> str:
        """åˆ—å‡ºOllamaæ¨¡å‹"""
        models = self.get_ollama_models()
        result = "ğŸ§  å¯ç”¨çš„Ollamaæ¨¡å‹:\n\n"
        for model in models:
            name = model.get("name", "")
            size = model.get("size", 0)
            size_gb = f"{size / 1000000000:.1f}GB" if size > 0 else "æœªçŸ¥å¤§å°"
            result += f"â€¢ **{name}** ({size_gb})\n"
        return result

    def get_config_instructions(self) -> str:
        """è·å–é…ç½®è¯´æ˜"""
        return """
ğŸ”§ Agenté…ç½®è¯´æ˜:

**è‡ªåŠ¨é…ç½®æ–¹å¼:**
ç›´æ¥é€‰æ‹©æƒ³è¦çš„Agentæ¨¡å¼å’Œæ¨¡å‹ç»„åˆï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é…ç½®ã€‚

**å¯ç”¨ç»„åˆ:**
- ğŸ”— Chain Agent + ä»»æ„Ollamaæ¨¡å‹
- ğŸ› ï¸ Tool Agent + ä»»æ„Ollamaæ¨¡å‹  
- ğŸ•¸ï¸ Graph Agent + ä»»æ„Ollamaæ¨¡å‹

**æ¨èé…ç½®:**
- æ—¥å¸¸å¯¹è¯: Chain Agent + qwen2.5:7b
- å¤æ‚ä»»åŠ¡: Tool Agent + qwen2.5:14b
- å·¥ä½œæµ: Graph Agent + llama3.1:8b

é€‰æ‹©æ¨¡å‹åï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é…ç½®ç›¸åº”çš„Agentæ¨¡å¼ï¼
"""

    def get_config_help(self) -> str:
        """è·å–å¸®åŠ©ä¿¡æ¯"""
        return """
ğŸ¯ LangChain Agenté…ç½®å™¨

**å¯ç”¨å‘½ä»¤:**
- "åˆ—å‡ºæ¨¡å¼" - æŸ¥çœ‹æ‰€æœ‰Agentæ¨¡å¼
- "åˆ—å‡ºæ¨¡å‹" - æŸ¥çœ‹æ‰€æœ‰Ollamaæ¨¡å‹
- "é…ç½®è¯´æ˜" - æŸ¥çœ‹é…ç½®æ–¹æ³•
- "å¸®åŠ©" - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

**å¿«é€Ÿå¼€å§‹:**
1. ä»æ¨¡å‹åˆ—è¡¨ä¸­é€‰æ‹©Agentæ¨¡å¼å’Œæ¨¡å‹ç»„åˆ
2. ç³»ç»Ÿè‡ªåŠ¨é…ç½®ç›¸åº”çš„Agent
3. å¼€å§‹å¯¹è¯ï¼

**Agentæ¨¡å¼:**
ğŸ”— Chain Agent - ç®€å•å¯¹è¯
ğŸ› ï¸ Tool Agent - å·¥å…·è°ƒç”¨
ğŸ•¸ï¸ Graph Agent - å¤æ‚å·¥ä½œæµ
"""
